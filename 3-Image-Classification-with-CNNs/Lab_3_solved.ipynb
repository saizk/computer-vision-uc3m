{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "l4_Image_Classification.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "YAS33NCoZcug"
   },
   "source": [
    "%matplotlib inline"
   ],
   "execution_count": 167,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2V4mVF5Zcuj"
   },
   "source": [
    "# AUTOMATIC DIAGNOSTIC SYSTEM OF SKIN LESIONS FROM DERMOSCOPIC IMAGES\n",
    "\n",
    "\n",
    "In this practice we are going to build a skin lesion diagnosis system based on dermoscopic image analysis.\n",
    "\n",
    "## Part 0: The problem\n",
    "\n",
    "Before starting the practice, we will briefly describe the database that we will use and the problem we aim to address:\n",
    "\n",
    "Our goal is to develop a CNN providing an automatic diagnosis of cutaneous diseases from dermoscopic images. Dermoscopy is a non-invasive technique that allows the evaluation of the colors and microstructures of the epidermis, the dermoepidermal joint and the papillary dermis that are not visible to the naked eye. These structures are specifically correlated with histological properties of the lesions. Identifying specific visual patterns related to color distribution or dermoscopic structures can help dermatologists decide the malignancy of a pigmented lesion. The use of this technique provides a great help to the experts to support their diagnosis. However, the complexity of its analysis limits its application to experienced clinicians or dermatologists.\n",
    "\n",
    "In our scenario, we will consider 3 classes of skin lesions:\n",
    "\n",
    "- Malignant melanoma: Melanoma, also known as malignant melanoma, is the most common type of cancer, and arises from pigmented cells known as melanocytes. Melanomas typically occur on the skin and rarely elsewhere such as the mouth, intestines, or eye.\n",
    "\n",
    "- Seborrheic keratosis: it is a noncancerous (benign) tumor of the skin that originates from the cells of the outer layer of the skin (keranocytes), so it is a non-melanocytic lesion.\n",
    "\n",
    "- Benign nevus: a benign skin tumor caused by melanocytes (it is melanocytic)\n",
    "\n",
    "Figure 1 shows a visual example of the 3 considered lesions:\n",
    "\n",
    "![Image of ISIC](http://www.tsc.uc3m.es/~igonzalez/images/ISIC.jpg)\n",
    "\n",
    "The dataset has been obtained from the 'International Skin Imaging Collaboration' (ISIC) file. It contains 2750 images divided into 3 sets:\n",
    "- Training set: 2000 images\n",
    "- Validation set: 150 images\n",
    "- Test set: 600 images\n",
    "\n",
    "For each clinical case, two images are available:\n",
    "- The dermoscopic image of the lesion (in the ‘images’ folder).\n",
    "- A binary mask with the segmentation between injury (mole) and skin (in the 'masks' folder)\n",
    "\n",
    "Additionally, there is a csv file for each dataset (training, validation and test) in which each lines corresponds with a clinical case, defined with two fields separated by commas:\n",
    "- the numerical id of the lesion: that allows to build the paths to the image and mask.\n",
    "- the lesion label: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1).\n",
    "\n",
    "Students will be able to use the training and validation sets to build their solutions and finally provide the scores associated with the test set. This practice provides a guideliness to build a baseline reference system. To do so, we will learn two fundamental procedures:\n",
    "\n",
    "- 1) Process your own database with pytorch\n",
    "- 2) Fine-tuning a regular network for our diagnostic problem\n",
    "\n",
    "## Part 1: Handling our custom dataset with pytorch\n",
    "Now we are going to study how we can load and process our custom dataset in pytorch. For that end, we are going to use the package ``scikit-image`` for reading images, and the package ``panda`` for reading csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZJJWNnbtZcul",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c59eee5c-32d5-49e7-a99a-a7dfa8ca3f93"
   },
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, util\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchvision import transforms, utils, models\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.models.detection as dmodels\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from PIL import Image\n",
    "import pdb\n",
    "import random\n",
    "import numpy.random as npr\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "npr.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "execution_count": 168,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCcFp7JgZcun"
   },
   "source": [
    "The first thing we need to do is to download and decompress the dataset on a local folder:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E-Q0lfUXZcun",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "outputId": "e6c480a3-3486-4325-875d-6e8f9c304141"
   },
   "source": [
    "#ONLY TO USE GOOGLE COLAB. Run this code only the first time you run this notebook and then comment these lines\n",
    "from shutil import copyfile\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "drive.mount('/content/drive')\n",
    "copyfile('/content/drive/My Drive/Colab Notebooks/db1.zip', './db1.zip') #Copy db files to our working folder\n",
    "copyfile('/content/drive/My Drive/Colab Notebooks/db2.zip', './db2.zip')\n"
   ],
   "execution_count": 150,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./db2.zip'"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CtMNWCnoZcuo"
   },
   "source": [
    "#NOTE: Run this only once, in the machine where you want to run your code, then comment these lines\n",
    "import zipfile\n",
    "zipPath='./db1.zip' #path of the 1st zip file\n",
    "dataFolder='./data' #We extract files to the current folder\n",
    "with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataFolder)\n",
    "\n",
    "zipPath='./db2.zip' #path of the 2nd zip file\n",
    "dataFolder='./data' # We extract files to the current folder\n",
    "with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataFolder)"
   ],
   "execution_count": 151,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W07_IPaLZcuo"
   },
   "source": [
    "Now let's read the indexed file and display data from image 65. The file structure is one row per image of the database, and two fields:\n",
    "- Image ID (a 4-digit sequence, adding 0 to the left side if required)\n",
    "- Label: 0 benign nevus, 1 melanoma, 2 seborrheic keratosis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8FO9qkL8Zcup"
   },
   "source": [
    "def clean_cache():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "execution_count": 152,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_HJb9cYMnhbk"
   },
   "source": [
    "# db = pd.read_csv('data/dermoscopyDBtrain.csv', header=0, dtype={'id': str, 'label': int})\n",
    "\n",
    "# #We show inform\n",
    "# n = 65\n",
    "# img_id = db.id[n] \n",
    "# label = db.label[n]\n",
    "\n",
    "\n",
    "# print(f'Image ID: {img_id}')\n",
    "# print(f'Label: {label}')"
   ],
   "execution_count": 153,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuELTstrZcup"
   },
   "source": [
    "Now, let's create a simple function to show an image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sr8iNsybZcuq"
   },
   "source": [
    "def imshow(image, title_str):\n",
    "    if len(image.shape)>2:\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image,cmap=plt.cm.gray)\n",
    "    plt.title(title_str)        \n",
    "\n",
    "# plt.figure()\n",
    "# imshow(io.imread(os.path.join('data/images/', img_id + '.jpg' )),'Image %d'%n)\n",
    "# plt.figure()\n",
    "# imshow(io.imread(os.path.join('data/masks/', img_id + '.png')),'Mask %d'%n)\n",
    "\n",
    "# plt.show()"
   ],
   "execution_count": 154,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW2D31E2Zcur"
   },
   "source": [
    "### Class Dataset\n",
    "\n",
    "The class `` torch.utils.data.Dataset`` is an abstract class that represents a dataset.\n",
    "\n",
    "To create our custom dataset in pytorch we must inherit from this class and overwrite the following methods:\n",
    "\n",
    "- `` __len__`` so that `` len (dataset) `` returns the size of the dataset.\n",
    "- `` __getitem__`` to support indexing `` dataset [i] `` when referring to sample $i$\n",
    "\n",
    "We are going to create the train and test datasets of our diagnostic problem. We will read the csv in the initialization method `` __init__`` but we will leave the explicit reading of the images for the method\n",
    "`` __getitem__``. This approach is more efficient in memory because all the images are not loaded in memory at first, but are read individually when necessary.\n",
    "\n",
    "Our dataset is going to be a dictionary `` {'image': image, 'mask': mask, 'label': label} ``. You can also take an optional `` transform '' argument so that we can add pre-processing and data augmentation techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OntgyPJtZcur"
   },
   "source": [
    "We now instantiate the class to iterate over some samples to see what we generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2IeUtP9UZcus",
    "scrolled": true
   },
   "source": [
    "# train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n",
    "#                                     root_dir='data')\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     sample = train_dataset[i]\n",
    "#     print(i, sample['image'].shape, sample['label'])\n",
    "\n",
    "#     ax = plt.subplot(1, 4, i + 1)\n",
    "#     plt.tight_layout()\n",
    "#     ax.set_title('Sample #{}'.format(i))\n",
    "#     ax.axis('off')\n",
    "#     plt.imshow(sample['image'])\n",
    "\n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break"
   ],
   "execution_count": 155,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu8_ataqZcus"
   },
   "source": [
    "The optional parameter ``maxSize`` in the constructor allows us to subsample the number of images and consequently reduce the size of the dataset. If not set included or maxSize=0, then the dataset will include all the images. This parameter is useful to train in smaller datasets during hyperparameter validation and decision making during design. Working with less images reduces the training time at the expense of obtaining results that may not correlate perfectly to what would happen with the full dataset size. Of course, the larger the trianing dataset, the more stable results but the larger the training time. Hence, it is up to the students the use of this parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kJE8wxUZcus"
   },
   "source": [
    "### Transforms\n",
    "----------\n",
    "\n",
    "In the previously shown examples we can see that the size of the images is not the same. This would prevent to train a red convolutional neuronal, as the vast majority require fixed-size inputs. Furthermore, the image is not always adjusted to the lesion, and indeed, in some examples lesions are very small compared to the size of the image. It would then be desirable to adjust the input images so that the lesion covers almost the entire image.\n",
    "\n",
    "To do this, we are going to create some preprocessing code, focusing on 4 transformations:\n",
    "\n",
    "- `` CropByMask``: to crop the image using the lesion mask\n",
    "- `` Rescale``: to scale the image\n",
    "- `` RandomCrop``: to crop the image randomly, it allows us to augment the data samples with random crops\n",
    "-  ``CenterCrop``: to perform a central crop of the image with the indicated size (useful in test)\n",
    "-  ``TVCenterCrop``: the same functionality of the previous one but using the method in torchvision. Just an example if you plan to use torchvision transforms.\n",
    "- `` ToTensor``: to convert numpy matrices into torch tensors (rearranging the axes).\n",
    "\n",
    "We will define them as callable classes instead of simple functions, as we will not need to pass the transform  parameters every time we call a method. To do this, we only have to implement the `` __call__`` method and, if necessary, the `` __init__`` method.\n",
    "Then we can use a transformation with the following code:\n",
    "\n",
    "::\n",
    "\n",
    "    tsfm = Transform(params)\n",
    "    transformed_sample = tsfm(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xQkXTqJrZcus"
   },
   "source": [
    "class CropByMask(object):\n",
    "    \"\"\"Crop the image using the lesion mask.\n",
    "\n",
    "    Args:\n",
    "        border (tuple or int): Border surrounding the mask. We dilate the mask as the skin surrounding \n",
    "        the lesion is important for dermatologists.\n",
    "        If it is a tuple, then it is (bordery,borderx)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, border):\n",
    "        assert isinstance(border, (int, tuple))\n",
    "        if isinstance(border, int):\n",
    "            self.border = (border,border)\n",
    "        else:\n",
    "            self.border = border\n",
    "            \n",
    "    def __call__(self, image, mask):\n",
    "\n",
    "        # h, w = image.size[:2]\n",
    "        h, w = image.shape[:2]\n",
    "        #Calculamos los índices del bounding box para hacer el cropping\n",
    "        sidx=np.nonzero(mask)\n",
    "        minx=np.maximum(sidx[1].min() - self.border[1], 0)\n",
    "        maxx=np.minimum(sidx[1].max() + 1 + self.border[1], w)\n",
    "        miny=np.maximum(sidx[0].min() - self.border[0], 0)\n",
    "        maxy=np.minimum(sidx[0].max() + 1 + self.border[1], h)\n",
    "        #Recortamos la imagen\n",
    "        \n",
    "        image=image[miny:maxy, minx:maxx,...]\n",
    "\n",
    "        # image = image.crop([minx, miny, maxx, maxy])\n",
    "\n",
    "        return image\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Re-scale image to a predefined size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size. \n",
    "        If it is an int, the smallest dimension will be the output_size\n",
    "            a we will keep fixed the original aspect ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        image = transform.resize(image, (int(new_h), int(new_w)))\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Randomly crop the image.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Crop size. If  int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        if h>new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top=0\n",
    "            \n",
    "        if w>new_w: \n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0\n",
    "            \n",
    "        image = image[top: top + new_h,\n",
    "                     left: left + new_w]\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "    \n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crop the central area of the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tupla or int): Crop size. If int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        rem_h = h - new_h\n",
    "        rem_w = w - new_w\n",
    "        \n",
    "        if h>new_h:\n",
    "            top = int(rem_h/2)\n",
    "        else:\n",
    "            top=0\n",
    "            \n",
    "        if w>new_w: \n",
    "            left = int(rem_w/2)\n",
    "        else:\n",
    "            left = 0\n",
    "            \n",
    "        image = image[top: top + new_h,\n",
    "                     left: left + new_w]\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "class TVCenterCrop(object):\n",
    "    \"\"\"Crop the central area of the image. Example using the method in torchvision. Requires to\n",
    "    internally convert from skimage (numpy array) to PIL Image\n",
    "\n",
    "    Args:\n",
    "        output_size (tupla or int): Crop size. If int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.CC=transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        pil_image=Image.fromarray(util.img_as_ubyte(image))\n",
    "        pil_image=self.CC(pil_image)\n",
    "        image=util.img_as_float(np.asarray(pil_image))\n",
    "        \n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        # Cambiamos los ejes\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        label=torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return {'image': image, 'label': label}\n",
    "    \n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n",
    "\n",
    "    Args:\n",
    "        mean_vec: Vector with means. \n",
    "        std_vec: Vector with standard deviations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean,std):\n",
    "      \n",
    "        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n",
    "        self.mean = np.array(mean)\n",
    "        self.std = np.array(std)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        c, h, w = image.shape\n",
    "        assert c == len(self.mean), 'Length of mean and image is not the same' \n",
    "        dtype = image.dtype\n",
    "        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n",
    "        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    \n",
    "        return {'image': image, 'label': label}\n"
   ],
   "execution_count": 156,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVGdfpNHZcuu"
   },
   "source": [
    "### Composed Transforms\n",
    "\n",
    "Now let's apply the different transformations to our images. \n",
    "\n",
    "We will rescale the images so that their smallest dimension is 256 and then make random crops of size 224. To compose the transformations ``Rescale`` and ``RandomCrop`` we can use ``torchvision.transforms.Compose``, which is a simple callable class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8ytQDFZhZcuv"
   },
   "source": [
    "class CropByMask2(object):\n",
    "\n",
    "    def __init__(self, border):\n",
    "        assert isinstance(border, (int, tuple))\n",
    "        if isinstance(border, int):\n",
    "            self.border = (border,border)\n",
    "        else:\n",
    "            self.border = border\n",
    "            \n",
    "    def __call__(self, image, mask):\n",
    "\n",
    "        h, w = image.size[:2]\n",
    "\n",
    "        #Calculamos los índices del bounding box para hacer el cropping\n",
    "        \n",
    "        sidx=np.nonzero(mask)\n",
    "        minx=np.maximum(sidx[1].min() - self.border[1], 0)\n",
    "        maxx=np.minimum(sidx[1].max() + 1 + self.border[1], w)\n",
    "        miny=np.maximum(sidx[0].min() - self.border[0], 0)\n",
    "        maxy=np.minimum(sidx[0].max() + 1 + self.border[1], h)\n",
    "        #Recortamos la imagen\n",
    "        \n",
    "        image = image.crop([minx, maxy, maxx, miny])\n",
    "\n",
    "        return image"
   ],
   "execution_count": 157,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOhTFZFutns2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpvJ_F7YZcuw"
   },
   "source": [
    "Iterating the dataset\n",
    "-----------------------------\n",
    "\n",
    "We can now put everything together to create the train and test datasets with the corresponding transformations.\n",
    "In summary, every time we sample an image from the dataset (during training):\n",
    "- We will read the image and the mask\n",
    "- We will apply the transformations and we will crop the image using a bounding box computed from the mask\n",
    "- As the final cropping operation is random, we perform data augmentation during sampling\n",
    "\n",
    "We can easily iterate over the dataset with a ``for i in range`` loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKoFtEURZcuw"
   },
   "source": [
    "Finally, we have to create a dataloader allowing to:\n",
    "\n",
    "- Sample batches of samples to feed the network during training\n",
    "- Shuffle data\n",
    "- Load the data in parallel using multiple cores.\n",
    "\n",
    "``torch.utils.data.DataLoader`` is an iterator that provides all these features. An important parameter of the iterator is ``collate_fn``. We can specify how samples are organized in batches by choosing the most appropriate function. In any case, the default option should work fine in most cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CyjZPwz3Zcuw",
    "scrolled": false
   },
   "source": [
    "# Auxiliary function to visualize a batch\n",
    "def show_batch(sample_batched):\n",
    "    \"\"\"Mostramos las lesiones de un batch.\"\"\"\n",
    "    images_batch, labels_batch = \\\n",
    "            sample_batched['image'], sample_batched['label']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "    grid_border_size = 2\n",
    "    \n",
    "    #Generamos el grid\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    #Lo pasamos a numpy y lo desnormalizamos\n",
    "    grid=grid.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    plt.title('Batch from dataloader')\n",
    "\n",
    "# for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "#     print(i_batch, sample_batched['image'].size(),\n",
    "#           sample_batched['label'])\n",
    "#     plt.figure()\n",
    "#     show_batch(sample_batched)\n",
    "#     plt.axis('off')\n",
    "#     plt.ioff()\n",
    "#     plt.show()\n",
    "        \n",
    "#     #We show the data of the 3rd batch and stop.\n",
    "#     if i_batch == 0:\n",
    "#         break"
   ],
   "execution_count": 158,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdLiVRj_Zcux"
   },
   "source": [
    "## Part 2: Fine-tuning a pre-trained model\n",
    "\n",
    "In the second part of the practice we will build an automatic skin lesion diagnosis system. Instead of training a CNN designed by us from the beginning, we will fine-tune a network that has previously been trained for another task. As seen in the lectures, this usually becomes a good alternative when we do not have many data in the training dataset (in relation to the parameters to be learned).\n",
    "\n",
    "In particular, we will use the Alexnet CNN, included in the ``torchvision`` package.\n",
    "\n",
    "### Performance Metric for evaluation\n",
    "We will start by defining the metric we will use to evaluate our network. In particular, and following the instructions of the organizers of the original ISIC challenge, we will use the area under the ROC or AUC (https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve), but we will calculate 3 different AUCs:\n",
    "- 1) AUC of binary problem melanoma vs all\n",
    "- 2) AUC of the binary problem seborrheic keratosis vs all\n",
    "- 3) AUC average of the previous two\n",
    "\n",
    "The following function computes AUCs from the complete database outputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tmkhSHRyZcux"
   },
   "source": [
    "#Function that computes 2 AUCs: melanoma vs all and keratosis vs all\n",
    "# scores is nx3: n is the number of samples in the dataset \n",
    "# labels is nx1\n",
    "# Function resturns an array with two elements: the auc values\n",
    "def computeAUCs(scores,labels):\n",
    "                \n",
    "    aucs = np.zeros((2,))\n",
    "    #Calculamos el AUC melanoma vs all\n",
    "    scores_mel = scores[:,1]\n",
    "    labels_mel = (labels == 1).astype(np.int) \n",
    "    aucs[0]=metrics.roc_auc_score(labels_mel, scores_mel)\n",
    "\n",
    "    #Calculamos el AUC queratosis vs all\n",
    "    scores_sk = scores[:,2]\n",
    "    labels_sk = (labels == 2).astype(np.int) \n",
    "    aucs[1]=metrics.roc_auc_score(labels_sk, scores_sk)\n",
    "    \n",
    "    return aucs"
   ],
   "execution_count": 159,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56AysvMhexhv"
   },
   "source": [
    "### Set-up new dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QSw4Dc7Wexhv"
   },
   "source": [
    "class IsicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, img_dir, mask_dir, transform=None, max_size=2000):\n",
    "        \n",
    "        self.dataset = pd.read_csv(csv_file, header=0, dtype={'id': str, 'label': int})\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        \n",
    "        self.img_names = self.dataset.id\n",
    "        self.labels = self.dataset.label\n",
    "\n",
    "        if max_size > 0:\n",
    "            idx = np.random.RandomState(seed=42).permutation(range(len(self.dataset)))\n",
    "            reduced_dataset = self.dataset.iloc[idx[0: max_size]]\n",
    "            self.dataset = reduced_dataset.reset_index(drop=True)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.classes = ['nevus', 'melanoma', 'keratosis']\n",
    "\n",
    "        self.img_files = [f'{img}.jpg' for img in self.img_names]\n",
    "        self.masks_files = [f'{mask}.png' for mask in self.img_names]        \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])       \n",
    "        mask_path = os.path.join(self.mask_dir, self.masks_files[idx])       \n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            if isinstance(self.transform.transforms[0], CropByMask2):\n",
    "                tr_func = self.transform.transforms[0]\n",
    "                image = tr_func(image, mask=Image.open(mask_path))\n",
    "                self.transform.transforms.pop(0)\n",
    "\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx].astype(\"int64\")\n",
    "\n",
    "        return image, label"
   ],
   "execution_count": 160,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ew1OEEDtZcur"
   },
   "source": [
    "class DermoscopyDataset(IsicDataset):\n",
    "    \"\"\"Dermoscopy dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path al fichero csv con las anotaciones.\n",
    "            root_dir (string): Directorio raíz donde encontraremos las carpetas 'images' y 'masks' .\n",
    "            transform (callable, optional): Transformaciones opcionales a realizar sobre las imágenes.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])       \n",
    "        mask_path = os.path.join(self.mask_dir, self.masks_files[idx])       \n",
    "        \n",
    "        image = io.imread(img_path)\n",
    "        \n",
    "        label = self.labels[idx].astype(\"int64\")\n",
    "\n",
    "        sample = {'image': image, 'label':  label}\n",
    "\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform.transforms[0], CropByMask):\n",
    "                tr_func = self.transform.transforms[0]\n",
    "                sample['image'] = tr_func(image, mask=io.imread(mask_path))\n",
    "                self.transform.transforms.pop(0)\n",
    "            \n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample['image'], sample['label']"
   ],
   "execution_count": 161,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rgAUGP4DZcuw"
   },
   "source": [
    "#Pixel means and stds expected by models in torchvision\n",
    "\n",
    "def setup_datasets(dataset, transform_compose, max_size=2000):\n",
    "\n",
    "    #Train Dataset\n",
    "    train_dataset = dataset(csv_file='data/dermoscopyDBtrain.csv',\n",
    "                              img_dir='data/images',\n",
    "                              mask_dir='data/masks',\n",
    "                              max_size=max_size,\n",
    "                              transform=transform_compose['train'])\n",
    "    #Val dataset\n",
    "    val_dataset = dataset(csv_file='data/dermoscopyDBval.csv',\n",
    "                              img_dir='data/images',\n",
    "                              mask_dir='data/masks',\n",
    "                              transform=transform_compose['val'])\n",
    "\n",
    "    #Test dataset\n",
    "    test_dataset = dataset(csv_file='data/dermoscopyDBtest.csv',\n",
    "                              img_dir='data/images',\n",
    "                              mask_dir='data/masks',\n",
    "                              transform=transform_compose['test'])\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "execution_count": 162,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH685diSZcuy"
   },
   "source": [
    "### Set-up the data loaders\n",
    "No we will assign the dataloaders over the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aHOLhg42uPY4"
   },
   "source": [
    "#Specify training dataset, with a batch size of 8, shuffle the samples, and parallelize with 4 workers\n",
    "\n",
    "def setup_dataloaders(dataset, transform, num_workers, batch_sizes, max_train_size):\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = setup_datasets(dataset, transform, max_train_size)\n",
    "        \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_sizes['train'], shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_sizes['val'], shuffle=False, num_workers=num_workers)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_sizes['test'], shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    \n",
    "    datasets = {'train' : train_dataset, 'val': val_dataset, 'test': test_dataset}\n",
    "    dataloaders = {'train' : train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
    "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}  # 'test': len(test_dataset)\n",
    "    \n",
    "    return datasets, dataloaders, dataset_sizes"
   ],
   "execution_count": 163,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDp7B7lPZcux"
   },
   "source": [
    "### Training function\n",
    "\n",
    "We continue defining the function to train our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tScnnZyOZcux"
   },
   "source": [
    "#train_model parameters are the network (model), the criterion (loss),\n",
    "# the optimizer, a learning scheduler (una estrategia de lr strategy), and the training epochs\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs,\n",
    "                dataset_sizes, dataloaders, device):\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc, best_auc = 0, 0\n",
    "    best_epoch = -1\n",
    "    \n",
    "    #Loop of epochs (each iteration involves train and val datasets)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Cada época tiene entrenamiento y validación\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set the model in training mode\n",
    "            else:\n",
    "                model.eval()   # Set the model in val mode (no grads)\n",
    "            \n",
    "            num_samples = dataset_sizes[phase]\n",
    "            \n",
    "            # Create variables to store outputs and labels\n",
    "            outputs_m = np.zeros((num_samples, num_classes), dtype=np.float)\n",
    "            labels_m = np.zeros((num_samples, ), dtype=np.int)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            cont_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                batch_size = labels.shape[0]\n",
    "\n",
    "                optimizer.zero_grad()  # Set grads to zero\n",
    "\n",
    "                # Forward\n",
    "                # Register ops only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward & parameters update only in train\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Accumulate the running loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                outputs = F.softmax(outputs.data, dim=1)\n",
    "                # Store outputs and labels \n",
    "                outputs_m[cont_samples: cont_samples + batch_size, ...] = outputs.cpu().numpy()\n",
    "                labels_m [cont_samples: cont_samples + batch_size]      = labels.cpu().numpy()\n",
    "                cont_samples += batch_size\n",
    "\n",
    "            #At the end of an epoch, update the lr scheduler    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            #Accumulated loss by epoch\n",
    "            epoch_loss = running_loss / num_samples\n",
    "            epoch_acc = running_corrects / num_samples\n",
    "\n",
    "            epoch_auc = computeAUCs(outputs_m, labels_m)\n",
    "            \n",
    "            print(f'{phase} -> Loss: {epoch_loss}, Avg. acc: {epoch_acc}\\\n",
    "                  Mel: {epoch_auc[0]} / Seb: {epoch_auc[1]}, Avg. AUC: {epoch_auc.mean()}')\n",
    "\n",
    "            # Deep copy of the best model\n",
    "            if phase == 'val' and epoch_acc > best_auc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_epoch = epoch\n",
    "                \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best model in epoch {:d} avg {:4f}'.format(best_epoch, best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "execution_count": 164,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiR60OYEZcux"
   },
   "source": [
    "### Fine-tuning of a pre-trained CNN\n",
    "Once we have defined the training and evaluation functions, we will fine-tune AlexNet CNN using our database. In addition, we define the loss, the optimizer and the lr scheduler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLQ-3f_zZcuz"
   },
   "source": [
    "## Part 3: Evaluation (Important)\n",
    "The evaluation of this practice will be done through a challenge. For this, students are asked to provide the following:\n",
    "- Two submissions for the test set scores (unnormalized), each one represented by a 600x3 matrix where 600 is the number of test samples, and 3 are the classes considered in the problem. The matrix must be provided in csv format (with 3 numbers per row separated by ',').\n",
    "\n",
    "In addition, students will submit a short report (1 side at most for the description, plus 1 side for references and 1 for figures, if necessary) where they will describe the most important aspects of the proposed solution and include a table with the validation results achieved by their extensions/decisions. The objective of this report is for the teacher to assess the developments / extensions / decisions made by the students when optimizing their system. And the table is asked to demonstrate that, at least in validation, their decisions helped to improve the system performance.  You don't need to provide an absolute level of detail about the changes made, just list them, briefly discuss their purpose and show their impact in the table.\n",
    "\n",
    "The deadline for delivery of the results file and the report is Wednesday Nov 3 at 23:59.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJYf25PEZcuz"
   },
   "source": [
    "Next we provide some functions that allow to test the network and create the csv file with the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ttrCPe0AZcuz"
   },
   "source": [
    "### Code that generates the test matrix\n",
    "\n",
    "def test_model(model, dataset, dataloader, device):\n",
    "    model.eval()  # Ponemos el modelo en modo evaluación\n",
    "\n",
    "    num_samples, num_classes = len(dataset), len(dataset.classes)  # Tamaño del dataset\n",
    "\n",
    "    # Creamos las variables que almacenarán las salidas y las etiquetas\n",
    "    outputs_m = np.zeros((num_samples, num_classes), dtype=np.float)\n",
    "    cont_samples = 0\n",
    "\n",
    "    # Iteramos sobre los datos\n",
    "    for inputs, _labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        batch_size = inputs.shape[0]  # Tamaño del batch\n",
    "\n",
    "        with torch.torch.no_grad():  # Paso forward\n",
    "            outputs = model(inputs)\n",
    "            outputs = F.softmax(outputs.data, dim=1)  # Aplicamos un softmax a la salida\n",
    "            outputs_m[cont_samples: cont_samples + batch_size, ...] = outputs.cpu().numpy()\n",
    "            cont_samples += batch_size\n",
    "\n",
    "    return outputs_m"
   ],
   "execution_count": 165,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKI4ZVYkZcuz"
   },
   "source": [
    "Running the previous function and obtaining the matrix with the scores (Nx3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A9SnfJEWZcuz"
   },
   "source": [
    "# outputs=test_model(model_ft)\n",
    "# print(outputs)"
   ],
   "execution_count": 166,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flCJwneLZcuz"
   },
   "source": [
    "And finally save the matrix into a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zPbEGFNSenJC"
   },
   "source": [
    "class BaseNet(object):\n",
    "\n",
    "    def __init__(self, model, transform_c, max_train_size, criterion, optimizer, scheduler, num_workers, batch_sizes,\n",
    "                 device):\n",
    "\n",
    "        self.datasets, self.dataloaders, self.sizes = self._setup(transform_c, num_workers, batch_sizes, max_train_size)\n",
    "        self.num_classes = len(self.datasets['train'].classes)\n",
    "\n",
    "        self.model = model(pretrained=True)\n",
    "        self.model_name = model.__name__.lower()\n",
    "        self.trained_model = None\n",
    "        self.model_path = None\n",
    "\n",
    "        self.criterion = criterion\n",
    "\n",
    "        optimizer_func = optimizer['func']\n",
    "        self.optimizer = optimizer_func(self.model.parameters(), **optimizer['args'])\n",
    "\n",
    "        scheduler_func = scheduler['func']\n",
    "        self.scheduler = scheduler_func(self.optimizer, **scheduler['args'])\n",
    "\n",
    "        self._adjust_network()\n",
    "\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _setup(transform_c, num_workers, batch_sizes, max_train_size):\n",
    "        return setup_dataloaders(transform_c['dataset'], transform_c, num_workers, batch_sizes, max_train_size)\n",
    "\n",
    "    def _adjust_network(self):\n",
    "\n",
    "        if isinstance(self.model, models.AlexNet) or isinstance(self.model, models.VGG): # or \\\n",
    "                # isinstance(self.model, models.EfficientNet):\n",
    "            num_features = self.model.classifier[-1].in_features\n",
    "            self.model.classifier[-1] = nn.Linear(num_features, self.num_classes)\n",
    "\n",
    "        elif isinstance(self.model, models.ResNet) or isinstance(self.model, models.ShuffleNetV2) or \\\n",
    "                isinstance(self.model, models.Inception3) or isinstance(self.model, models.GoogLeNet):\n",
    "            num_ftrs = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_ftrs, self.num_classes)\n",
    "\n",
    "        elif isinstance(self.model, dmodels.MaskRCNN):\n",
    "            self.model.roi_heads.mask_predictor = dmodels.mask_rcnn.MaskRCNNPredictor(256, 256, self.num_classes)\n",
    "\n",
    "        elif isinstance(self.model, dmodels.FasterRCNN):\n",
    "            in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "            self.model.roi_heads.box_predictor = dmodels.faster_rcnn.FastRCNNPredictor(in_features, self.num_classes)\n",
    "\n",
    "    def trainloop(self, num_epochs=25, path='/content/drive/MyDrive'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.trained_model = train_model(\n",
    "            self.model, self.criterion, self.optimizer, self.scheduler,\n",
    "            num_epochs=num_epochs,\n",
    "            dataloaders={'train': self.dataloaders['train'], 'val': self.dataloaders['val']},\n",
    "            dataset_sizes=self.sizes,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.model_path = f'{path}/{self.model_name}_model_n{num_epochs}.pt'\n",
    "        self.save_model(self.model_path)\n",
    "\n",
    "        return self.trained_model\n",
    "\n",
    "    def evaluate(self, output_path, dataset='test', model_path=None, save_to_csv=False):\n",
    "        clean_cache()\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "\n",
    "        # model = self.load_model(model_path)\n",
    "        model = self.trained_model\n",
    "\n",
    "        outputs = test_model(model, self.datasets[dataset], self.dataloaders[dataset], device=self.device)\n",
    "\n",
    "        if save_to_csv:\n",
    "            self.save_output_to_csv(outputs, path=f'{output_path}/{self.model_name}_output_{dataset}.csv')\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def save_model(self, path):\n",
    "        model_scripted = torch.jit.script(self.trained_model)\n",
    "        model_scripted.save(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model):\n",
    "        return torch.jit.load(model)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_output_to_csv(outputs, path):\n",
    "        import csv\n",
    "        with open(path, mode='w', newline='') as out_file:\n",
    "            csv_writer = csv.writer(out_file, delimiter=',')\n",
    "            csv_writer.writerows(outputs)"
   ],
   "execution_count": 169,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sG13OyZQith1"
   },
   "source": [
    "def get_data_transforms(mode='torch'):\n",
    "    original_c = transforms.Compose([\n",
    "            CropByMask(15),\n",
    "            Rescale(256),\n",
    "            CenterCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    )\n",
    "    train_composed = transforms.Compose([\n",
    "            CropByMask2((25, 25)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Resize((256, 256)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "    )\n",
    "    test_composed = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "    )\n",
    "    if mode == 'torch':\n",
    "      return {'train': train_composed, 'val': train_composed, 'test': test_composed, \n",
    "              'dataset': IsicDataset}\n",
    "    \n",
    "    elif mode == 'original':\n",
    "      return {'train': original_c, 'val': original_c, 'test': original_c,\n",
    "              'dataset': DermoscopyDataset}\n",
    "\n",
    "def gen_model(params, device):\n",
    "    return BaseNet(\n",
    "        model=params.get('model'),\n",
    "        transform_c=params.get('transform'),\n",
    "        max_train_size=params.get('max_train_size'),\n",
    "        criterion=params.get('criterion'),\n",
    "        optimizer=params.get('optimizer'),\n",
    "        scheduler=params.get('scheduler'),\n",
    "        num_workers=params.get('num_workers'),\n",
    "        batch_sizes=params.get('batch_sizes'),\n",
    "        device=device\n",
    "    )"
   ],
   "execution_count": 170,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1Wz5VJxm9ezz"
   },
   "source": [
    "def gen_model_params(model, data_transforms, max_train_size):\n",
    "    return {\n",
    "        'model': model,\n",
    "        'transform': data_transforms,\n",
    "        'max_train_size': max_train_size,\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'optimizer':\n",
    "            {'func': optim.SGD,\n",
    "             'args': {'lr': 0.001, 'momentum': 0.9}},\n",
    "        'scheduler':\n",
    "            {'func': lr_scheduler.StepLR,\n",
    "             'args': {'step_size': 7, 'gamma': 0.1}},\n",
    "        'num_workers': 8,\n",
    "        'batch_sizes': {\n",
    "            'train': 64,\n",
    "            'val': 128,\n",
    "            'test': 128\n",
    "        }\n",
    "    }"
   ],
   "execution_count": 171,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_tsRiMIoqMB",
    "outputId": "baf5a8b6-f98b-4f47-b25d-8ddc602f803d"
   },
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_transforms = get_data_transforms(mode='torch')\n",
    "\n",
    "detection_models = [dmodels.fasterrcnn_resnet50_fpn,\n",
    "                    dmodels.maskrcnn_resnet50_fpn]\n",
    "\n",
    "test_models = [\n",
    "      # models.vgg19_bn,\n",
    "      # models.vgg16_bn,\n",
    "    # models.efficientnet_b7,\n",
    "    # models.resnext101_32x8d,\n",
    "    # models.resnext50_32x4d,\n",
    "\n",
    "    # models.alexnet,\n",
    "    # models.resnet18,\n",
    "    # models.resnet50,\n",
    "    models.resnet101,\n",
    "    # models.resnext50_32x4d,\n",
    "    # models.resnext101_32x8d,\n",
    "    # models.googlenet,\n",
    "    # models.vgg19_bn,\n",
    "    # models.vgg19,\n",
    "    # models.shufflenet_v2_x0_5\n",
    "]\n",
    "\n",
    "model_params = [gen_model_params(model, data_transforms, max_train_size=2000) for model in test_models]\n",
    "print(device)"
   ],
   "execution_count": 172,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTCkflMkexhz",
    "outputId": "891279fd-5237-414e-c974-b87f97e758d2"
   },
   "source": [
    "net_models = [gen_model(params, device) for params in model_params]\n",
    "\n",
    "for model in net_models:\n",
    "    model_trained = model.trainloop(num_epochs=25)\n",
    "    outputs = model.evaluate(output_path='/content/drive/MyDrive', save_to_csv=True)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "----------\n",
      "train -> Loss: 1.2118875522613526, Avg. acc: 0.23650000989437103                  Mel: 0.5785991014990364 / Seb: 0.6181823921494349, Avg. AUC: 0.5983907468242357\n",
      "val -> Loss: 1.0478185844421386, Avg. acc: 0.4866666793823242                  Mel: 0.6916666666666667 / Seb: 0.7458112874779541, Avg. AUC: 0.7187389770723104\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n",
      "train -> Loss: 1.071602681159973, Avg. acc: 0.45250001549720764                  Mel: 0.7207970742809032 / Seb: 0.8194703754814153, Avg. AUC: 0.7701337248811593\n",
      "val -> Loss: 1.0135192569096882, Avg. acc: 0.54666668176651                  Mel: 0.7575000000000001 / Seb: 0.8247354497354498, Avg. AUC: 0.7911177248677249\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n",
      "train -> Loss: 0.9145900378227234, Avg. acc: 0.6330000162124634                  Mel: 0.7894787905098303 / Seb: 0.8848571763581099, Avg. AUC: 0.83716798343397\n",
      "val -> Loss: 0.8883570194244385, Avg. acc: 0.6600000262260437                  Mel: 0.7444444444444445 / Seb: 0.9186507936507937, Avg. AUC: 0.8315476190476191\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n",
      "train -> Loss: 0.7419798054695129, Avg. acc: 0.7565000653266907                  Mel: 0.838189250876466 / Seb: 0.9186599742042555, Avg. AUC: 0.8784246125403607\n",
      "val -> Loss: 0.8370365651448568, Avg. acc: 0.6333333253860474                  Mel: 0.6858333333333333 / Seb: 0.9074074074074074, Avg. AUC: 0.7966203703703704\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n",
      "train -> Loss: 0.5837511506080627, Avg. acc: 0.8205000162124634                  Mel: 0.8846403036222875 / Seb: 0.9427938775694276, Avg. AUC: 0.9137170905958576\n",
      "val -> Loss: 0.742835009098053, Avg. acc: 0.6866666674613953                  Mel: 0.7830555555555555 / Seb: 0.9058641975308642, Avg. AUC: 0.8444598765432099\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n",
      "train -> Loss: 0.41421393513679505, Avg. acc: 0.8850000500679016                  Mel: 0.9427698956133947 / Seb: 0.967606497641403, Avg. AUC: 0.955188196627399\n",
      "val -> Loss: 0.7024816139539083, Avg. acc: 0.7200000286102295                  Mel: 0.7941666666666667 / Seb: 0.9314373897707231, Avg. AUC: 0.8628020282186949\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n",
      "train -> Loss: 0.2782927498817444, Avg. acc: 0.9415000677108765                  Mel: 0.9756217481960916 / Seb: 0.9868089942365452, Avg. AUC: 0.9812153712163184\n",
      "val -> Loss: 0.7640265043576558, Avg. acc: 0.6866666674613953                  Mel: 0.8419444444444444 / Seb: 0.906305114638448, Avg. AUC: 0.8741247795414462\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n",
      "train -> Loss: 0.1867891914844513, Avg. acc: 0.968500018119812                  Mel: 0.9939765574126329 / Seb: 0.9959953459425819, Avg. AUC: 0.9949859516776074\n",
      "val -> Loss: 0.7648153726259868, Avg. acc: 0.6800000071525574                  Mel: 0.7433333333333332 / Seb: 0.9263668430335097, Avg. AUC: 0.8348500881834214\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n",
      "train -> Loss: 0.16117812967300416, Avg. acc: 0.9760000705718994                  Mel: 0.9954548743348396 / Seb: 0.9975106204507942, Avg. AUC: 0.9964827473928168\n",
      "val -> Loss: 0.7446122399965922, Avg. acc: 0.6800000071525574                  Mel: 0.7352777777777778 / Seb: 0.927689594356261, Avg. AUC: 0.8314836860670194\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n",
      "train -> Loss: 0.15172701168060304, Avg. acc: 0.9785000681877136                  Mel: 0.9968295939643889 / Seb: 0.9975399337969352, Avg. AUC: 0.997184763880662\n",
      "val -> Loss: 0.7373066051801046, Avg. acc: 0.699999988079071                  Mel: 0.7422222222222222 / Seb: 0.9404761904761905, Avg. AUC: 0.8413492063492063\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n",
      "train -> Loss: 0.144470406293869, Avg. acc: 0.9795000553131104                  Mel: 0.9972406943320771 / Seb: 0.9982276699948589, Avg. AUC: 0.997734182163468\n",
      "val -> Loss: 0.7549460331598917, Avg. acc: 0.6933333277702332                  Mel: 0.735 / Seb: 0.9422398589065256, Avg. AUC: 0.8386199294532628\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n",
      "train -> Loss: 0.13250867307186126, Avg. acc: 0.9845000505447388                  Mel: 0.9985397714939716 / Seb: 0.9990349144501267, Avg. AUC: 0.9987873429720491\n",
      "val -> Loss: 0.7452991716066997, Avg. acc: 0.7066667079925537                  Mel: 0.7477777777777778 / Seb: 0.9435626102292769, Avg. AUC: 0.8456701940035274\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n",
      "train -> Loss: 0.12311539244651794, Avg. acc: 0.987000048160553                  Mel: 0.9979708085850914 / Seb: 0.9989875621217451, Avg. AUC: 0.9984791853534183\n",
      "val -> Loss: 0.7489115897814432, Avg. acc: 0.7266666889190674                  Mel: 0.7427777777777778 / Seb: 0.9417989417989417, Avg. AUC: 0.8422883597883597\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n",
      "train -> Loss: 0.12636135506629945, Avg. acc: 0.984000027179718                  Mel: 0.9985759483263282 / Seb: 0.999075502160168, Avg. AUC: 0.9988257252432482\n",
      "val -> Loss: 0.7352910089492798, Avg. acc: 0.7400000095367432                  Mel: 0.7491666666666666 / Seb: 0.9455467372134039, Avg. AUC: 0.8473567019400352\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n",
      "train -> Loss: 0.11222666239738464, Avg. acc: 0.9875000715255737                  Mel: 0.9990248699278437 / Seb: 0.999276185837595, Avg. AUC: 0.9991505278827193\n",
      "val -> Loss: 0.7473309175173442, Avg. acc: 0.7266666889190674                  Mel: 0.7408333333333333 / Seb: 0.9426807760141094, Avg. AUC: 0.8417570546737214\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n",
      "train -> Loss: 0.11457557106018067, Avg. acc: 0.9910000562667847                  Mel: 0.9993504614190527 / Seb: 0.9992987345653959, Avg. AUC: 0.9993245979922243\n",
      "val -> Loss: 0.7480806954701742, Avg. acc: 0.7266666889190674                  Mel: 0.7294444444444445 / Seb: 0.947530864197531, Avg. AUC: 0.8384876543209877\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n",
      "train -> Loss: 0.11432062470912933, Avg. acc: 0.9860000610351562                  Mel: 0.9991597108484453 / Seb: 0.9991093252518692, Avg. AUC: 0.9991345180501573\n",
      "val -> Loss: 0.7421183260281881, Avg. acc: 0.7266666889190674                  Mel: 0.7369444444444444 / Seb: 0.9484126984126985, Avg. AUC: 0.8426785714285714\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_jpI8ozbexhz"
   },
   "source": [
    "outputs = model.evaluate(output_path='/content/drive/MyDrive', save_to_csv=True)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}